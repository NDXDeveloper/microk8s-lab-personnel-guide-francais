üîù Retour au [Sommaire](/SOMMAIRE.md)

# 8.11 Alerting avec Prometheus Alertmanager

## Introduction √† l'alerting

L'alerting est un composant essentiel de toute strat√©gie de monitoring. Avoir de beaux dashboards Grafana c'est bien, mais personne ne peut les regarder 24h/24. C'est l√† qu'intervient Alertmanager : il surveille constamment vos m√©triques et vous pr√©vient quand quelque chose ne va pas.

Dans un environnement Kubernetes sur MicroK8s, l'alerting devient crucial. Vos pods peuvent red√©marrer, manquer de m√©moire, ou vos volumes peuvent se remplir - autant de situations o√π vous voulez √™tre pr√©venu imm√©diatement plut√¥t que de d√©couvrir le probl√®me quand les utilisateurs se plaignent.

## Architecture de l'alerting Prometheus

### Vue d'ensemble du syst√®me

Le syst√®me d'alerting Prometheus fonctionne en deux parties distinctes mais compl√©mentaires :

**Prometheus** √©value les r√®gles d'alerte et g√©n√®re des alertes quand les conditions sont remplies. Il ne g√®re pas l'envoi des notifications lui-m√™me.

**Alertmanager** re√ßoit les alertes de Prometheus, les d√©duplique, les groupe, les route vers les bons destinataires et g√®re les silences. C'est le chef d'orchestre des notifications.

Cette s√©paration permet une grande flexibilit√© : vous pouvez avoir plusieurs Prometheus qui envoient vers un seul Alertmanager, ou l'inverse selon vos besoins.

### Flux de donn√©es

```
M√©triques ‚Üí Prometheus ‚Üí R√®gles d'alerte ‚Üí Alertes ‚Üí Alertmanager ‚Üí Notifications
                ‚Üë                                           ‚Üì
            Recording                                   Email/Slack/
              Rules                                     PagerDuty/etc.
```

## Installation d'Alertmanager sur MicroK8s

### Pr√©requis

Avant d'installer Alertmanager, assurez-vous que Prometheus est d√©j√† install√© et fonctionnel dans votre cluster MicroK8s. Alertmanager a besoin de Prometheus pour recevoir les alertes.

### D√©ploiement via manifeste

Cr√©ez d'abord un namespace d√©di√© si ce n'est pas d√©j√† fait :

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
```

### ConfigMap pour la configuration

La configuration d'Alertmanager se fait via un ConfigMap :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default-receiver'

    receivers:
    - name: 'default-receiver'
      # Configuration du receiver (email, slack, etc.)
```

### D√©ploiement d'Alertmanager

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:latest
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: alertmanager-storage
          mountPath: /alertmanager
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: alertmanager-storage
        emptyDir: {}
```

### Service pour exposer Alertmanager

```yaml
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
    - port: 9093
      targetPort: 9093
  type: ClusterIP
```

## Configuration de Prometheus pour Alertmanager

### Connexion √† Alertmanager

Dans votre configuration Prometheus, ajoutez la section alerting :

```yaml
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.monitoring.svc.cluster.local:9093
```

### Chargement des r√®gles d'alerte

```yaml
rule_files:
  - '/etc/prometheus/rules/*.yml'
```

## Cr√©ation de r√®gles d'alerte

### Anatomie d'une r√®gle d'alerte

Une r√®gle d'alerte Prometheus suit cette structure :

```yaml
groups:
- name: example-alerts
  interval: 30s
  rules:
  - alert: HighMemoryUsage
    expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) < 0.1
    for: 5m
    labels:
      severity: warning
      component: infrastructure
    annotations:
      summary: "M√©moire disponible faible sur {{ $labels.instance }}"
      description: "Moins de 10% de m√©moire disponible depuis 5 minutes"
```

### Composants d'une r√®gle

**alert** : Le nom de l'alerte, unique et descriptif

**expr** : La requ√™te PromQL qui d√©finit la condition d'alerte. Si elle retourne un r√©sultat, l'alerte se d√©clenche

**for** : Dur√©e pendant laquelle la condition doit √™tre vraie avant de d√©clencher l'alerte. √âvite les faux positifs

**labels** : M√©tadonn√©es additionnelles attach√©es √† l'alerte. Utilis√©es pour le routage et le groupement

**annotations** : Informations descriptives pour enrichir l'alerte. Peuvent utiliser des templates

## R√®gles d'alerte essentielles pour Kubernetes

### Alertes sur les Pods

```yaml
groups:
- name: kubernetes-pods
  rules:
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} red√©marre en boucle"
      description: "Le pod a red√©marr√© {{ $value }} fois dans les 15 derni√®res minutes"

  - alert: PodNotReady
    expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) > 0
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} n'est pas pr√™t"
      description: "Le pod est en √©tat {{ $labels.phase }} depuis 15 minutes"

  - alert: ContainerOOMKilled
    expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} > 0
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "Container tu√© pour manque de m√©moire"
      description: "{{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} a √©t√© tu√© (OOM)"
```

### Alertes sur les Nodes

```yaml
groups:
- name: kubernetes-nodes
  rules:
  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.node }} n'est pas pr√™t"
      description: "Le node Kubernetes n'est pas dans un √©tat Ready"

  - alert: NodeMemoryPressure
    expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pression m√©moire sur le node {{ $labels.node }}"
      description: "Le node manque de m√©moire disponible"

  - alert: NodeDiskPressure
    expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pression disque sur le node {{ $labels.node }}"
      description: "Le node manque d'espace disque"
```

### Alertes sur les Deployments

```yaml
groups:
- name: kubernetes-deployments
  rules:
  - alert: DeploymentReplicasMismatch
    expr: |
      kube_deployment_spec_replicas != kube_deployment_status_replicas_available
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
      description: "Nombre de replicas d√©sir√©: {{ $labels.spec_replicas }}, disponible: {{ $value }}"

  - alert: DeploymentGenerationMismatch
    expr: |
      kube_deployment_status_observed_generation != kube_deployment_metadata_generation
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} generation mismatch"
      description: "Le deployment a une configuration non appliqu√©e"
```

### Alertes sur les ressources

```yaml
groups:
- name: resource-usage
  rules:
  - alert: HighCPUUsage
    expr: |
      sum(rate(container_cpu_usage_seconds_total[5m])) by (pod, namespace) > 0.8
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Utilisation CPU √©lev√©e pour {{ $labels.namespace }}/{{ $labels.pod }}"
      description: "CPU √† {{ $value | humanizePercentage }} depuis 10 minutes"

  - alert: HighMemoryUsage
    expr: |
      sum(container_memory_working_set_bytes) by (pod, namespace) /
      sum(container_spec_memory_limit_bytes) by (pod, namespace) > 0.8
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Utilisation m√©moire √©lev√©e pour {{ $labels.namespace }}/{{ $labels.pod }}"
      description: "M√©moire √† {{ $value | humanizePercentage }} de la limite"

  - alert: PersistentVolumeFillingUp
    expr: |
      kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} presque plein"
      description: "Moins de 10% d'espace disponible"
```

## Configuration d'Alertmanager

### Structure du fichier de configuration

Le fichier `alertmanager.yml` est structur√© en plusieurs sections :

```yaml
global:
  # Configuration globale
  resolve_timeout: 5m
  smtp_from: 'alertmanager@example.com'
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_auth_username: 'alertmanager@example.com'
  smtp_auth_password: 'your-app-password'

templates:
  # Chemins vers les templates personnalis√©s
  - '/etc/alertmanager/templates/*.tmpl'

route:
  # Routage des alertes
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'team-devops'

  routes:
  - match:
      severity: critical
    receiver: 'team-oncall'
    continue: true

  - match:
      severity: warning
    receiver: 'team-devops'

receivers:
  # D√©finition des receivers
  - name: 'team-devops'
  - name: 'team-oncall'

inhibit_rules:
  # R√®gles d'inhibition
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

### Concepts cl√©s de routage

**group_by** : Regroupe les alertes similaires en une seule notification. √âvite le spam

**group_wait** : Temps d'attente avant d'envoyer la premi√®re notification d'un nouveau groupe

**group_interval** : Temps d'attente avant d'envoyer les alertes suivantes du m√™me groupe

**repeat_interval** : Intervalle de r√©p√©tition pour les alertes non r√©solues

## Configuration des receivers

### Email

Configuration pour envoyer des emails via Gmail :

```yaml
receivers:
- name: 'email-notifications'
  email_configs:
  - to: 'devops-team@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'smtp.gmail.com:587'
    auth_username: 'alertmanager@example.com'
    auth_password: 'app-specific-password'
    headers:
      Subject: '[{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}'
    html: |
      <h2>üö® Alerte Kubernetes</h2>
      <p><b>Cluster:</b> MicroK8s Lab</p>
      {{ range .Alerts }}
      <hr>
      <p><b>Alerte:</b> {{ .Labels.alertname }}</p>
      <p><b>S√©v√©rit√©:</b> {{ .Labels.severity }}</p>
      <p><b>Description:</b> {{ .Annotations.description }}</p>
      <p><b>Depuis:</b> {{ .StartsAt.Format "2006-01-02 15:04:05" }}</p>
      {{ end }}
```

### Slack

Configuration pour Slack :

```yaml
receivers:
- name: 'slack-notifications'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#alerts'
    title: 'Alerte Kubernetes'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    send_resolved: true
    color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
    fields:
    - title: 'S√©v√©rit√©'
      value: '{{ .GroupLabels.severity }}'
      short: true
    - title: 'Cluster'
      value: 'MicroK8s Lab'
      short: true
```

### Webhook g√©n√©rique

Pour int√©gration avec d'autres syst√®mes :

```yaml
receivers:
- name: 'webhook-receiver'
  webhook_configs:
  - url: 'http://example.com/alerts'
    send_resolved: true
    http_config:
      bearer_token: 'your-auth-token'
    max_alerts: 10
```

### PagerDuty

Pour les alertes critiques n√©cessitant une escalade :

```yaml
receivers:
- name: 'pagerduty-critical'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
    description: '{{ .GroupLabels.alertname }}'
    details:
      severity: '{{ .GroupLabels.severity }}'
      namespace: '{{ .CommonLabels.namespace }}'
      pod: '{{ .CommonLabels.pod }}'
```

## Routage avanc√© des alertes

### Routage par s√©v√©rit√©

```yaml
route:
  receiver: 'default'
  group_by: ['alertname']

  routes:
  - match:
      severity: critical
    receiver: 'pagerduty-oncall'
    group_wait: 30s
    repeat_interval: 4h

  - match:
      severity: warning
    receiver: 'slack-warnings'
    group_wait: 5m
    repeat_interval: 12h

  - match_re:
      namespace: kube-.*
    receiver: 'k8s-admin-team'
```

### Routage par namespace

```yaml
route:
  receiver: 'default'

  routes:
  - match:
      namespace: production
    receiver: 'prod-team'
    routes:
    - match:
        severity: critical
      receiver: 'prod-oncall'

  - match:
      namespace: staging
    receiver: 'staging-team'

  - match:
      namespace: development
    receiver: 'dev-team'
```

### Routage temporel (heures de bureau)

Bien qu'Alertmanager ne supporte pas nativement les horaires, vous pouvez utiliser des labels dans Prometheus :

```yaml
groups:
- name: business-hours
  rules:
  - alert: NonCriticalIssue
    expr: some_metric > threshold
    labels:
      severity: warning
      time_sensitivity: business_hours
    annotations:
      summary: "Probl√®me non critique d√©tect√©"
```

Puis router diff√©remment :

```yaml
routes:
- match:
    time_sensitivity: business_hours
  receiver: 'email-only'

- match:
    time_sensitivity: '24x7'
  receiver: 'pagerduty'
```

## Inhibition et silence

### R√®gles d'inhibition

Les inhibitions emp√™chent certaines alertes d'√™tre envoy√©es si d'autres sont d√©j√† actives :

```yaml
inhibit_rules:
- source_match:
    severity: 'critical'
    alertname: 'NodeDown'
  target_match:
    severity: 'warning'
  equal: ['node']
  # Si un node est down, on n'envoie pas les alertes warning pour ce node

- source_match:
    alertname: 'DeploymentDown'
  target_match:
    alertname: 'PodDown'
  equal: ['namespace', 'deployment']
  # Si un deployment est down, on n'envoie pas les alertes pour ses pods
```

### Cr√©ation de silences

Les silences peuvent √™tre cr√©√©es via l'interface web d'Alertmanager ou via l'API :

```bash
# Via amtool (CLI d'Alertmanager)
amtool silence add alertname="TestAlert" --duration="2h" --comment="Maintenance planifi√©e"

# Via l'API REST
curl -X POST http://alertmanager:9093/api/v1/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {"name": "alertname", "value": "DiskSpaceLow"},
      {"name": "instance", "value": "node1"}
    ],
    "startsAt": "2024-01-01T00:00:00Z",
    "endsAt": "2024-01-01T02:00:00Z",
    "createdBy": "admin",
    "comment": "Maintenance disque"
  }'
```

## Templates personnalis√©s

### Cr√©ation de templates

Cr√©ez un fichier `/etc/alertmanager/templates/custom.tmpl` :

```go
{{ define "custom.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "custom.slack.text" }}
{{ range .Alerts }}
*Alerte:* {{ .Labels.alertname }}
*Namespace:* {{ .Labels.namespace }}
*Pod:* {{ .Labels.pod }}
*Description:* {{ .Annotations.description }}
*Graphique:* <http://grafana.example.com/d/dashboard-id?var-namespace={{ .Labels.namespace }}|Voir dans Grafana>
{{ end }}
{{ end }}

{{ define "custom.email.html" }}
<!DOCTYPE html>
<html>
<head>
    <style>
        .alert-box {
            border: 2px solid #ff6b6b;
            padding: 10px;
            margin: 10px 0;
            border-radius: 5px;
        }
        .resolved {
            border-color: #51cf66;
        }
    </style>
</head>
<body>
    <h1>{{ template "custom.title" . }}</h1>
    {{ range .Alerts }}
    <div class="alert-box {{ if eq .Status "resolved" }}resolved{{ end }}">
        <h3>{{ .Labels.alertname }}</h3>
        <p><strong>Status:</strong> {{ .Status }}</p>
        <p><strong>D√©but:</strong> {{ .StartsAt.Format "02/01/2006 15:04:05" }}</p>
        {{ if .EndsAt }}
        <p><strong>Fin:</strong> {{ .EndsAt.Format "02/01/2006 15:04:05" }}</p>
        {{ end }}
        <p>{{ .Annotations.description }}</p>
    </div>
    {{ end }}
</body>
</html>
{{ end }}
```

### Utilisation des templates

```yaml
receivers:
- name: 'custom-email'
  email_configs:
  - to: 'team@example.com'
    html: '{{ template "custom.email.html" . }}'
    headers:
      Subject: '{{ template "custom.title" . }}'

- name: 'custom-slack'
  slack_configs:
  - api_url: 'YOUR_WEBHOOK_URL'
    title: '{{ template "custom.title" . }}'
    text: '{{ template "custom.slack.text" . }}'
```

## Int√©gration avec Grafana

### Configuration de Grafana pour Alertmanager

Dans Grafana, ajoutez Alertmanager comme source de donn√©es :

1. Configuration ‚Üí Data Sources ‚Üí Add data source
2. S√©lectionnez "Alertmanager"
3. URL : `http://alertmanager.monitoring.svc.cluster.local:9093`
4. Sauvegardez et testez

### Visualisation des alertes dans Grafana

Grafana peut afficher les alertes actives d'Alertmanager :

1. Cr√©ez un nouveau panel
2. Choisissez la visualisation "Alert list"
3. S√©lectionnez votre datasource Alertmanager
4. Configurez les filtres si n√©cessaire

### Annotations d'alertes sur les graphiques

Ajoutez des annotations pour voir quand les alertes se sont d√©clench√©es :

```json
{
  "datasource": "Prometheus",
  "enable": true,
  "expr": "ALERTS{alertname=\"HighCPUUsage\"}",
  "iconColor": "red",
  "name": "High CPU Alerts",
  "step": "60s",
  "tagKeys": "severity,namespace",
  "textFormat": "{{ alertname }}"
}
```

## Monitoring d'Alertmanager

### M√©triques d'Alertmanager

Alertmanager expose ses propres m√©triques :

```promql
# Nombre d'alertes re√ßues
rate(alertmanager_alerts_received_total[5m])

# Nombre de notifications envoy√©es
rate(alertmanager_notifications_sent_total[5m])

# Erreurs d'envoi de notifications
rate(alertmanager_notifications_failed_total[5m])

# Nombre de silences actifs
alertmanager_silences
```

### Alertes sur Alertmanager lui-m√™me

```yaml
groups:
- name: alertmanager-monitoring
  rules:
  - alert: AlertmanagerNotificationsFailing
    expr: rate(alertmanager_notifications_failed_total[5m]) > 0
    for: 5m
    labels:
      severity: critical
      component: alerting
    annotations:
      summary: "Alertmanager n'arrive pas √† envoyer les notifications"
      description: "{{ $value }} notifications ont √©chou√© dans les 5 derni√®res minutes"

  - alert: AlertmanagerConfigReloadFailed
    expr: alertmanager_config_last_reload_successful == 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Rechargement de la configuration Alertmanager √©chou√©"
      description: "La derni√®re tentative de rechargement a √©chou√©"
```

## Bonnes pratiques

### Conception des alertes

**√âvitez le bruit** : Utilisez le param√®tre `for` pour √©viter les alertes transitoires
```yaml
for: 5m  # L'alerte doit √™tre vraie pendant 5 minutes
```

**Soyez sp√©cifique** : Incluez suffisamment de labels pour identifier le probl√®me
```yaml
labels:
  namespace: "{{ $labels.namespace }}"
  pod: "{{ $labels.pod }}"
  container: "{{ $labels.container }}"
```

**Messages actionnables** : Les annotations doivent dire quoi faire
```yaml
annotations:
  summary: "Espace disque faible sur {{ $labels.node }}"
  description: "Il reste {{ $value | humanize }}% d'espace"
  runbook_url: "https://wiki.example.com/runbooks/disk-space"
```

### Organisation des r√®gles

**Groupez logiquement** : Organisez vos r√®gles par composant
```
rules/
‚îú‚îÄ‚îÄ kubernetes-pods.yml
‚îú‚îÄ‚îÄ kubernetes-nodes.yml
‚îú‚îÄ‚îÄ kubernetes-storage.yml
‚îú‚îÄ‚îÄ application-specific.yml
‚îî‚îÄ‚îÄ infrastructure.yml
```

**Versionnez** : Gardez vos r√®gles dans Git pour tracer les changements

**Testez** : Validez vos r√®gles avant de les d√©ployer
```bash
promtool check rules rules/*.yml
```

### Gestion de la fatigue d'alerte

**Groupement intelligent** : Groupez les alertes li√©es
```yaml
group_by: ['alertname', 'namespace', 'deployment']
```

**Intervalles appropri√©s** : Ajustez selon la criticit√©
```yaml
routes:
- match:
    severity: critical
  repeat_interval: 4h
- match:
    severity: warning
  repeat_interval: 24h
```

**Inhibitions pertinentes** : √âvitez les alertes redondantes
```yaml
inhibit_rules:
- source_match:
    alertname: ClusterDown
  target_match_re:
    alertname: .*
  equal: ['cluster']
```

## D√©pannage courant

### Les alertes ne se d√©clenchent pas

**V√©rifiez la r√®gle** : Testez la requ√™te PromQL dans Prometheus
```
http://prometheus:9090/graph
```

**V√©rifiez la configuration** : Les r√®gles sont-elles charg√©es ?
```bash
curl http://prometheus:9090/api/v1/rules | jq
```

**V√©rifiez les logs** : Prometheus indique les erreurs d'√©valuation
```bash
kubectl logs -n monitoring prometheus-0 | grep error
```

### Les notifications ne sont pas envoy√©es

**V√©rifiez Alertmanager** : L'alerte arrive-t-elle ?
```
http://alertmanager:9093/#/alerts
```

**V√©rifiez le routage** : La route est-elle correcte ?
```bash
amtool config routes --config.file=alertmanager.yml
```

**V√©rifiez les logs** : Erreurs d'envoi ?
```bash
kubectl logs -n monitoring alertmanager-0 | grep error
```

### Trop d'alertes (spam)

**Augmentez le `for`** : Attendez plus longtemps avant d'alerter
```yaml
for: 10m  # Au lieu de 1m
```

**Ajustez les seuils** : Sont-ils trop sensibles ?
```yaml
expr: cpu_usage > 0.9  # Au lieu de > 0.7
```

**Am√©liorez le groupement** : Regroupez mieux les alertes
```yaml
group_by: ['alertname', 'namespace', 'severity']
group_interval: 5m
```

## Conclusion

L'alerting avec Prometheus et Alertmanager est un pilier fondamental de l'observabilit√© dans Kubernetes. Dans votre lab MicroK8s, un syst√®me d'alerting bien configur√© vous permet de d√©tecter et r√©soudre les probl√®mes avant qu'ils n'impactent vos applications.

Commencez avec quelques alertes essentielles (pods crashant, nodes down, espace disque) puis enrichissez progressivement selon vos besoins. N'oubliez pas que trop d'alertes est aussi probl√©matique que pas assez - visez la qualit√© plut√¥t que la quantit√©.

L'objectif final est d'avoir un syst√®me qui vous alerte uniquement quand une action humaine est n√©cessaire, avec suffisamment de contexte pour r√©soudre rapidement le probl√®me. Avec le temps et l'exp√©rience, vous affinerez vos r√®gles pour atteindre cet √©quilibre optimal.

‚è≠Ô∏è
