üîù Retour au [Sommaire](/SOMMAIRE.md)

# 16.2 Analyse des logs

## Introduction √† l'importance des logs

Les logs sont la bo√Æte noire de votre cluster Kubernetes. Ils enregistrent tout ce qui se passe : d√©marrages d'applications, erreurs, requ√™tes trait√©es, probl√®mes de connexion, et bien plus encore. Savoir o√π trouver les logs, comment les lire et les interpr√©ter est une comp√©tence fondamentale pour diagnostiquer efficacement les probl√®mes dans MicroK8s. Cette section vous guidera √† travers les diff√©rents types de logs, leurs emplacements, et les techniques pour les analyser efficacement.

## Architecture des logs dans Kubernetes

### Comprendre les niveaux de logs

Dans un cluster MicroK8s, les logs existent √† plusieurs niveaux, chacun fournissant des informations diff√©rentes et compl√©mentaires :

**Logs d'application** : Ce sont les logs g√©n√©r√©s par vos applications d√©ploy√©es dans les pods. Ils contiennent les messages que votre application √©crit sur stdout (sortie standard) et stderr (erreur standard). Ces logs sont g√©n√©ralement les premiers √† consulter quand une application ne fonctionne pas correctement.

**Logs de conteneur** : Le runtime des conteneurs (containerd dans MicroK8s) g√©n√®re ses propres logs concernant le cycle de vie des conteneurs : cr√©ation, d√©marrage, arr√™t, erreurs de montage de volumes, etc.

**Logs Kubernetes** : Les composants de Kubernetes eux-m√™mes (API server, scheduler, controller manager, kubelet) produisent des logs d√©taillant leurs op√©rations. Dans MicroK8s, ces composants sont int√©gr√©s dans le processus kubelite.

**Logs syst√®me** : Au niveau du syst√®me d'exploitation, systemd (ou autre syst√®me d'init) capture les logs des services, incluant MicroK8s lui-m√™me.

### Cycle de vie et r√©tention des logs

Il est important de comprendre que les logs dans Kubernetes ne sont pas √©ternels. Par d√©faut :

- Les logs des pods sont conserv√©s tant que le pod existe
- Quand un conteneur red√©marre, les logs pr√©c√©dents sont accessibles avec l'option `--previous`
- Quand un pod est supprim√©, ses logs disparaissent √©galement
- Les logs syst√®me sont soumis √† la rotation configur√©e par votre syst√®me (g√©n√©ralement via logrotate ou journald)

Cette nature √©ph√©m√®re des logs souligne l'importance d'une strat√©gie de centralisation des logs pour les environnements de production, m√™me si pour un lab personnel, les logs locaux sont souvent suffisants.

## Acc√®s aux logs des applications

### Logs basiques des pods

La commande la plus fondamentale pour acc√©der aux logs d'une application est :

```bash
microk8s kubectl logs <nom-du-pod> -n <namespace>
```

Cette commande simple cache plusieurs subtilit√©s importantes. Par d√©faut, elle affiche les logs du conteneur principal du pod. Si votre pod contient plusieurs conteneurs (par exemple, un conteneur d'application et un sidecar pour la collecte de m√©triques), vous devez sp√©cifier le conteneur :

```bash
microk8s kubectl logs <nom-du-pod> -c <nom-du-conteneur> -n <namespace>
```

### Suivre les logs en temps r√©el

Pour surveiller une application en cours d'ex√©cution, l'option `-f` (follow) est essentielle :

```bash
microk8s kubectl logs -f <nom-du-pod> -n <namespace>
```

Cette commande fonctionne comme `tail -f` sous Linux : elle affiche les nouvelles lignes de log au fur et √† mesure qu'elles sont g√©n√©r√©es. C'est particuli√®rement utile pour :
- Surveiller le d√©marrage d'une application
- Observer le comportement lors de tests
- Diagnostiquer des probl√®mes intermittents

Pour arr√™ter le suivi, utilisez Ctrl+C.

### Gestion du volume de logs

Les applications peuvent g√©n√©rer √©norm√©ment de logs. Pour √©viter d'√™tre submerg√©, plusieurs options sont disponibles :

```bash
# Afficher seulement les 100 derni√®res lignes
microk8s kubectl logs <nom-du-pod> -n <namespace> --tail=100

# Afficher les logs depuis les 5 derni√®res minutes
microk8s kubectl logs <nom-du-pod> -n <namespace> --since=5m

# Afficher les logs depuis une heure sp√©cifique
microk8s kubectl logs <nom-du-pod> -n <namespace> --since-time=2024-01-15T10:00:00Z

# Combiner les options pour plus de pr√©cision
microk8s kubectl logs <nom-du-pod> -n <namespace> --tail=50 --since=10m
```

### Logs de pods avec plusieurs r√©plicas

Pour les d√©ploiements avec plusieurs r√©plicas, r√©cup√©rer les logs de tous les pods peut √™tre fastidieux. Utilisez les labels pour simplifier :

```bash
# Logs de tous les pods avec un label sp√©cifique
microk8s kubectl logs -l app=nginx -n <namespace>

# Suivre les logs de tous les pods d'un d√©ploiement
microk8s kubectl logs -f -l app=nginx -n <namespace> --prefix=true
```

L'option `--prefix=true` ajoute le nom du pod avant chaque ligne de log, facilitant l'identification de la source quand vous surveillez plusieurs pods simultan√©ment.

### Logs des conteneurs red√©marr√©s

Quand un conteneur crash et red√©marre, les logs du crash sont cruciaux pour comprendre le probl√®me :

```bash
# Voir les logs du conteneur pr√©c√©dent (avant le dernier red√©marrage)
microk8s kubectl logs <nom-du-pod> -n <namespace> --previous

# Si plusieurs conteneurs, sp√©cifier lequel
microk8s kubectl logs <nom-du-pod> -c <conteneur> -n <namespace> --previous
```

Cette commande est vitale quand vous voyez un pod avec un statut "CrashLoopBackOff" - elle vous permet de voir pourquoi le conteneur crash avant qu'il ne red√©marre.

## Logs des composants Kubernetes

### Logs du syst√®me MicroK8s

MicroK8s utilise systemd pour g√©rer ses services. Pour acc√©der aux logs syst√®me de MicroK8s :

```bash
# Logs du daemon principal MicroK8s
journalctl -u snap.microk8s.daemon-kubelite -f

# Voir les logs des derni√®res 2 heures
journalctl -u snap.microk8s.daemon-kubelite --since "2 hours ago"

# Logs avec plus de d√©tails (verbosit√© augment√©e)
journalctl -u snap.microk8s.daemon-kubelite -xe

# Filtrer par priorit√© (erreurs seulement)
journalctl -u snap.microk8s.daemon-kubelite -p err
```

### Logs des composants du control plane

Dans MicroK8s, les composants du control plane sont int√©gr√©s, mais vous pouvez toujours acc√©der √† leurs logs sp√©cifiques :

```bash
# Logs de l'API server (int√©gr√© dans kubelite)
journalctl -u snap.microk8s.daemon-kubelite | grep apiserver

# Logs du scheduler
journalctl -u snap.microk8s.daemon-kubelite | grep scheduler

# Logs du controller manager
journalctl -u snap.microk8s.daemon-kubelite | grep controller
```

### Logs des addons syst√®me

Les addons comme CoreDNS, ingress controller, ou dashboard s'ex√©cutent comme des pods dans le namespace `kube-system` :

```bash
# Logs de CoreDNS
microk8s kubectl logs -n kube-system -l k8s-app=kube-dns

# Logs de l'ingress controller
microk8s kubectl logs -n ingress -l name=nginx-ingress-microk8s

# Logs du dashboard
microk8s kubectl logs -n kube-system -l k8s-app=kubernetes-dashboard
```

## Interpr√©tation des logs

### Structure typique des logs

Les logs Kubernetes suivent g√©n√©ralement un format structur√©. Voici les √©l√©ments communs √† rechercher :

**Timestamp** : La plupart des logs commencent par un horodatage. Pour les afficher m√™me si l'application ne les inclut pas :

```bash
microk8s kubectl logs <pod> --timestamps
```

**Niveau de log** : Les niveaux standards sont :
- `DEBUG` : Informations d√©taill√©es pour le d√©bogage
- `INFO` : Informations g√©n√©rales sur le fonctionnement normal
- `WARNING`/`WARN` : Situations inhabituelles mais non critiques
- `ERROR` : Erreurs qui n√©cessitent attention
- `FATAL`/`CRITICAL` : Erreurs graves causant l'arr√™t de l'application

**Message** : Le contenu principal d√©crivant l'√©v√©nement

**Contexte** : Informations additionnelles comme l'ID de requ√™te, l'utilisateur, etc.

### Patterns d'erreurs courants

Reconna√Ætre les patterns d'erreurs courants acc√©l√®re consid√©rablement le diagnostic. Voici les plus fr√©quents :

**Erreurs de connexion r√©seau** :
```
dial tcp: lookup <service>: no such host
connection refused
timeout waiting for connection
network is unreachable
```
Ces erreurs indiquent g√©n√©ralement des probl√®mes DNS, des services non d√©marr√©s, ou des probl√®mes de configuration r√©seau.

**Erreurs de permissions** :
```
permission denied
forbidden: User cannot
cannot create resource
operation not permitted
```
Ces messages sugg√®rent des probl√®mes RBAC ou des permissions syst√®me insuffisantes.

**Erreurs de ressources** :
```
OOMKilled (Out Of Memory)
insufficient cpu
disk pressure
cannot allocate memory
```
Ces erreurs indiquent que les limites de ressources sont atteintes ou mal configur√©es.

**Erreurs de configuration** :
```
invalid configuration
missing required field
unknown field
validation error
```
Ces messages pointent vers des erreurs dans les manifestes YAML ou les ConfigMaps.

### Techniques de filtrage et recherche

Pour analyser efficacement de gros volumes de logs, ma√Ætrisez les outils de filtrage :

```bash
# Rechercher un terme sp√©cifique
microk8s kubectl logs <pod> | grep ERROR

# Recherche insensible √† la casse
microk8s kubectl logs <pod> | grep -i error

# Exclure certaines lignes
microk8s kubectl logs <pod> | grep -v DEBUG

# Recherche avec contexte (3 lignes avant et apr√®s)
microk8s kubectl logs <pod> | grep -C 3 "connection refused"

# Compter les occurrences
microk8s kubectl logs <pod> | grep -c ERROR

# Utiliser awk pour des analyses plus complexes
microk8s kubectl logs <pod> | awk '/ERROR/ {print $1, $2, $NF}'
```

## Logs avanc√©s et debugging

### Augmenter la verbosit√©

Pour obtenir plus d'informations lors du debugging, vous pouvez augmenter la verbosit√© des commandes kubectl :

```bash
# Niveau de verbosit√© de 0 √† 9
microk8s kubectl get pods -v=6

# Tr√®s verbeux (niveau 9) - affiche les requ√™tes HTTP
microk8s kubectl get pods -v=9
```

Cette technique est utile pour diagnostiquer les probl√®mes de communication avec l'API server.

### Logs d'√©v√©nements Kubernetes

Les √©v√©nements Kubernetes compl√®tent les logs en fournissant une vue de haut niveau des actions dans le cluster :

```bash
# √âv√©nements r√©cents tri√©s par timestamp
microk8s kubectl get events --all-namespaces --sort-by='.lastTimestamp'

# √âv√©nements pour un pod sp√©cifique
microk8s kubectl get events --field-selector involvedObject.name=<pod-name>

# Surveiller les √©v√©nements en temps r√©el
microk8s kubectl get events -w --all-namespaces
```

Les √©v√©nements sont particuli√®rement utiles pour comprendre pourquoi un pod ne d√©marre pas ou pourquoi un volume ne se monte pas.

### Debugging avec ephemeral containers

Pour les cas complexes, vous pouvez attacher un conteneur de debug √† un pod en cours d'ex√©cution :

```bash
# Attacher un conteneur de debug
microk8s kubectl debug <pod-name> -it --image=busybox

# Avec des outils r√©seau
microk8s kubectl debug <pod-name> -it --image=nicolaka/netshoot
```

Cette technique permet d'inspecter l'environnement du pod sans modifier l'application.

## Organisation et gestion des logs

### Sauvegarde des logs pour analyse

Pour une analyse approfondie ou pour partager avec d'autres, sauvegardez les logs :

```bash
# Sauvegarder dans un fichier
microk8s kubectl logs <pod> > pod_logs_$(date +%Y%m%d_%H%M%S).txt

# Sauvegarder avec m√©tadonn√©es
echo "=== Pod Description ===" > diagnostic.txt
microk8s kubectl describe pod <pod> >> diagnostic.txt
echo -e "\n=== Pod Logs ===" >> diagnostic.txt
microk8s kubectl logs <pod> >> diagnostic.txt
```

### Rotation et nettoyage des logs

Pour √©viter que les logs syst√®me ne remplissent votre disque :

```bash
# V√©rifier l'espace utilis√© par journald
journalctl --disk-usage

# Nettoyer les logs de plus de 7 jours
sudo journalctl --vacuum-time=7d

# Limiter la taille totale des logs
sudo journalctl --vacuum-size=1G

# Configuration permanente dans /etc/systemd/journald.conf
# SystemMaxUse=1G
# MaxRetentionSec=1week
```

### Scripts d'analyse automatis√©e

Cr√©ez des scripts pour automatiser l'analyse des patterns courants :

```bash
#!/bin/bash
# Script d'analyse des erreurs

NAMESPACE=${1:-default}
SINCE=${2:-1h}

echo "Analyse des erreurs dans le namespace $NAMESPACE (derni√®res $SINCE)"
echo "================================================"

echo -e "\n### Pods en erreur ###"
microk8s kubectl get pods -n $NAMESPACE | grep -v Running | grep -v Completed

echo -e "\n### R√©sum√© des erreurs par pod ###"
for pod in $(microk8s kubectl get pods -n $NAMESPACE -o name); do
    pod_name=$(echo $pod | cut -d'/' -f2)
    error_count=$(microk8s kubectl logs $pod_name -n $NAMESPACE --since=$SINCE 2>/dev/null | grep -c ERROR)
    if [ $error_count -gt 0 ]; then
        echo "$pod_name: $error_count erreurs"
    fi
done

echo -e "\n### Top 10 des messages d'erreur ###"
microk8s kubectl logs --all-containers=true -n $NAMESPACE --since=$SINCE 2>/dev/null | \
    grep ERROR | \
    sed 's/^[^ ]* [^ ]* //' | \
    sort | uniq -c | sort -rn | head -10
```

## Corr√©lation des logs

### Chronologie des √©v√©nements

Pour comprendre une s√©quence d'√©v√©nements, synchronisez les timestamps :

```bash
# Combiner logs et √©v√©nements avec timestamps
(
    echo "=== EVENTS ==="
    microk8s kubectl get events --sort-by='.lastTimestamp' | tail -20
    echo -e "\n=== POD LOGS ==="
    microk8s kubectl logs <pod> --timestamps --tail=50
) | sort -k1,2
```

### Tra√ßage des requ√™tes

Pour suivre une requ√™te √† travers plusieurs services, recherchez des identifiants communs :

```bash
# Si vos apps utilisent des request IDs
REQUEST_ID="abc-123-def"
for pod in $(microk8s kubectl get pods -o name); do
    echo "Checking $pod..."
    microk8s kubectl logs $pod | grep $REQUEST_ID
done
```

### Logs multi-composants

Pour des probl√®mes complexes impliquant plusieurs composants :

```bash
# Cr√©er une vue consolid√©e
{
    echo "=== Ingress Controller ==="
    microk8s kubectl logs -n ingress -l name=nginx-ingress-microk8s --tail=20
    echo -e "\n=== Application ==="
    microk8s kubectl logs -l app=myapp --tail=20
    echo -e "\n=== Database ==="
    microk8s kubectl logs -l app=postgres --tail=20
} > consolidated_logs.txt
```

## Outils et techniques compl√©mentaires

### Utilisation de stern pour les logs multi-pods

Stern est un outil tiers tr√®s utile pour suivre les logs de plusieurs pods :

```bash
# Installation
sudo snap install stern

# Suivre tous les pods d'un namespace
stern . -n <namespace>

# Suivre avec un pattern
stern "nginx-*" --since 15m

# Avec coloration par pod
stern . -n <namespace> --color always
```

### Analyse avec jq pour les logs JSON

Si vos applications produisent des logs JSON :

```bash
# Parser et formater les logs JSON
microk8s kubectl logs <pod> | jq '.'

# Filtrer par niveau
microk8s kubectl logs <pod> | jq 'select(.level == "error")'

# Extraire des champs sp√©cifiques
microk8s kubectl logs <pod> | jq '{time: .timestamp, msg: .message, level: .level}'

# Compter par type d'erreur
microk8s kubectl logs <pod> | jq -r '.error_type' | sort | uniq -c
```

## Bonnes pratiques pour les logs

### Configuration des applications

Pour faciliter l'analyse des logs, configurez vos applications pour :

1. **Utiliser des niveaux de log coh√©rents** : Adoptez une convention (DEBUG, INFO, WARN, ERROR) et respectez-la
2. **Inclure des timestamps** : M√™me si Kubernetes peut les ajouter, c'est mieux dans l'application
3. **Structurer les logs** : Pr√©f√©rez JSON ou un format structur√© au texte libre
4. **Inclure le contexte** : User ID, request ID, transaction ID facilitent le tra√ßage
5. **√âviter les logs sensibles** : Pas de mots de passe, tokens, ou donn√©es personnelles

### Strat√©gie de r√©tention

Pour un lab personnel, une strat√©gie simple mais efficace :

1. **Logs syst√®me** : 7-30 jours selon l'espace disponible
2. **Logs d'application** : Exporter les logs importants avant suppression des pods
3. **Logs de debug** : Activer temporairement, d√©sactiver apr√®s r√©solution
4. **Archives** : Compresser et archiver les logs d'incidents pour r√©f√©rence future

## R√©solution de probl√®mes courants via les logs

### Pod en CrashLoopBackOff

Quand un pod red√©marre continuellement :

```bash
# 1. V√©rifier le statut d√©taill√©
microk8s kubectl describe pod <pod-name>

# 2. Voir les logs du dernier crash
microk8s kubectl logs <pod-name> --previous

# 3. Chercher les patterns d'erreur
microk8s kubectl logs <pod-name> --previous | grep -E "ERROR|FATAL|Exception|Error:"

# 4. V√©rifier les √©v√©nements
microk8s kubectl get events --field-selector involvedObject.name=<pod-name>
```

### Service inaccessible

Pour diagnostiquer pourquoi un service ne r√©pond pas :

```bash
# 1. V√©rifier que les pods backend sont running
microk8s kubectl get endpoints <service-name>

# 2. Logs de l'ingress controller
microk8s kubectl logs -n ingress -l name=nginx-ingress-microk8s | grep <service-name>

# 3. Logs des pods du service
microk8s kubectl logs -l app=<app-label>

# 4. Tester depuis un pod de debug
microk8s kubectl run debug --image=nicolaka/netshoot --rm -it -- /bin/bash
# Dans le pod: curl http://<service-name>.<namespace>.svc.cluster.local
```

### Probl√®mes de performance

Pour identifier les goulots d'√©tranglement :

```bash
# 1. Analyser les temps de r√©ponse dans les logs
microk8s kubectl logs <pod> | grep "response_time" | awk '{print $NF}' | sort -n | tail -20

# 2. Chercher les timeouts
microk8s kubectl logs <pod> | grep -i timeout

# 3. V√©rifier les erreurs de ressources
microk8s kubectl describe pod <pod> | grep -A 5 "Events:"
```

## Conclusion

L'analyse des logs est un art qui s'affine avec la pratique. Les logs sont votre fen√™tre sur le comportement interne de votre cluster et de vos applications. En ma√Ætrisant les techniques pr√©sent√©es dans cette section, vous serez capable de diagnostiquer la grande majorit√© des probl√®mes que vous rencontrerez. Rappelez-vous que les logs racontent une histoire - votre r√¥le est d'apprendre √† la lire efficacement. La section suivante sur les probl√®mes r√©seau courants s'appuiera sur ces comp√©tences d'analyse des logs pour r√©soudre une cat√©gorie sp√©cifique mais fr√©quente de probl√®mes.

‚è≠Ô∏è
