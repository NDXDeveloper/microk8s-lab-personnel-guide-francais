üîù Retour au [Sommaire](/SOMMAIRE.md)

# 7.1 - Anatomie d'un d√©ploiement Kubernetes

## Introduction

Un d√©ploiement Kubernetes est comme un orchestre symphonique : plusieurs composants travaillent ensemble harmonieusement pour faire fonctionner votre application. Comprendre comment ces pi√®ces s'assemblent est essentiel pour ma√Ætriser Kubernetes. Dans cette section, nous allons diss√©quer un d√©ploiement typique pour comprendre chaque composant et son r√¥le.

## Vue d'ensemble de l'architecture

Quand vous d√©ployez une application sur Kubernetes, vous ne cr√©ez pas simplement un conteneur qui tourne. Vous mettez en place tout un √©cosyst√®me de ressources qui collaborent :

```
Internet ‚Üí Ingress ‚Üí Service ‚Üí Deployment ‚Üí ReplicaSet ‚Üí Pod(s) ‚Üí Container(s)
                        ‚Üì
                  ConfigMap/Secret
                        ‚Üì
                 PersistentVolume
```

Chaque fl√®che repr√©sente une relation ou un flux de communication. Voyons maintenant chaque composant en d√©tail.

## Les composants fondamentaux

### 1. Le Pod : l'unit√© de base

Le **Pod** est l'unit√© atomique de Kubernetes, la plus petite chose que vous pouvez d√©ployer. Pensez √† un Pod comme une "machine virtuelle logique" qui contient :

- **Un ou plusieurs conteneurs** (g√©n√©ralement un seul)
- **Un r√©seau partag√©** (m√™me adresse IP pour tous les conteneurs du Pod)
- **Un stockage partag√©** (volumes accessibles par tous les conteneurs)
- **Une identit√© unique** dans le cluster

**Caract√©ristiques importantes d'un Pod :**
- Il est √©ph√©m√®re par nature (peut dispara√Ætre √† tout moment)
- Il a une seule adresse IP
- Les conteneurs √† l'int√©rieur peuvent communiquer via `localhost`
- Il vit sur un seul n≈ìud (ne peut pas √™tre r√©parti sur plusieurs machines)

**Exemple de d√©finition d'un Pod simple :**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-application
  labels:
    app: webapp
spec:
  containers:
  - name: app-container
    image: nginx:latest
    ports:
    - containerPort: 80
```

**Pourquoi on ne d√©ploie jamais directement des Pods :**
En pratique, vous ne cr√©ez presque jamais de Pods directement. Pourquoi ? Parce qu'ils sont mortels. Si un Pod crash ou si le n≈ìud sur lequel il tourne a un probl√®me, le Pod dispara√Æt et ne revient pas. C'est l√† qu'intervient le Deployment.

### 2. Le Deployment : le gestionnaire intelligent

Le **Deployment** est votre chef d'orchestre. Il s'assure que votre application tourne toujours avec le bon nombre d'instances (replicas) et g√®re les mises √† jour.

**Responsabilit√©s du Deployment :**
- Maintenir le nombre d√©sir√© de Pods en fonctionnement
- Remplacer les Pods d√©faillants automatiquement
- G√©rer les mont√©es de version (rolling updates)
- Permettre les rollbacks en cas de probl√®me
- Distribuer les Pods sur diff√©rents n≈ìuds pour la r√©silience

**Structure d'un Deployment :**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
  labels:
    app: webapp
spec:
  replicas: 3  # Nombre de Pods d√©sir√©s
  selector:
    matchLabels:
      app: webapp  # S√©lectionne les Pods avec ce label
  template:
    metadata:
      labels:
        app: webapp  # Labels appliqu√©s aux Pods cr√©√©s
    spec:
      containers:
      - name: webapp
        image: myapp:v1.0
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
```

**Le ReplicaSet : le bras droit du Deployment**

Quand vous cr√©ez un Deployment, il cr√©e automatiquement un **ReplicaSet**. Le ReplicaSet est responsable de maintenir le nombre exact de Pods sp√©cifi√©. Si vous demandez 3 replicas et qu'un Pod meurt, le ReplicaSet en cr√©e imm√©diatement un nouveau.

Vous n'interagissez g√©n√©ralement pas directement avec les ReplicaSets, mais il est important de savoir qu'ils existent pour comprendre la hi√©rarchie :
- Deployment ‚Üí g√®re ‚Üí ReplicaSet ‚Üí g√®re ‚Üí Pods

### 3. Le Service : l'annuaire t√©l√©phonique

Les Pods sont √©ph√©m√®res et leurs IPs changent constamment. Comment faire pour acc√©der √† votre application de mani√®re stable ? C'est le r√¥le du **Service**.

**Le Service offre :**
- Une adresse IP stable (ClusterIP) qui ne change jamais
- Un nom DNS interne (ex: `webapp-service.default.svc.cluster.local`)
- Du load balancing automatique entre les Pods
- La d√©couverte de service (service discovery)

**Types de Services :**

**ClusterIP (par d√©faut)** : Accessible uniquement depuis l'int√©rieur du cluster
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: ClusterIP
  selector:
    app: webapp  # S√©lectionne les Pods avec ce label
  ports:
  - port: 80        # Port du Service
    targetPort: 8080 # Port du conteneur
    protocol: TCP
```

**NodePort** : Expose le service sur un port de chaque n≈ìud (30000-32767)
```yaml
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080  # Accessible via <IP-du-n≈ìud>:30080
```

**LoadBalancer** : Demande un load balancer externe (cloud provider ou MetalLB)
```yaml
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
```

### 4. L'Ingress : la porte d'entr√©e

L'**Ingress** est votre r√©ceptionniste. Il re√ßoit le trafic HTTP/HTTPS externe et le dirige vers les bons Services bas√© sur des r√®gles (nom de domaine, chemin URL).

**Fonctionnalit√©s de l'Ingress :**
- Routage bas√© sur le nom d'h√¥te (`app1.example.com` ‚Üí Service A)
- Routage bas√© sur le chemin (`/api` ‚Üí Service API, `/web` ‚Üí Service Web)
- Terminaison SSL/TLS (HTTPS)
- Load balancing
- H√¥tes virtuels

**Exemple d'Ingress :**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt"
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - webapp.monlab.local
    secretName: webapp-tls
  rules:
  - host: webapp.monlab.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webapp-service
            port:
              number: 80
```

### 5. ConfigMap : la configuration externalis√©e

Un **ConfigMap** stocke des donn√©es de configuration non sensibles que vos applications peuvent utiliser.

**Utilit√©s du ConfigMap :**
- Fichiers de configuration
- Variables d'environnement
- Arguments de ligne de commande
- URLs d'API, param√®tres d'application

**Cr√©ation d'un ConfigMap :**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  # Propri√©t√©s simples
  database_url: "postgres://db.monlab.local:5432/myapp"
  log_level: "info"

  # Fichier de configuration complet
  app.properties: |
    server.port=8080
    app.name=MonApplication
    cache.enabled=true
```

**Utilisation dans un Pod :**
```yaml
spec:
  containers:
  - name: webapp
    image: myapp:v1.0
    env:
    - name: DATABASE_URL
      valueFrom:
        configMapKeyRef:
          name: webapp-config
          key: database_url
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: webapp-config
```

### 6. Secret : les donn√©es sensibles

Un **Secret** est similaire √† un ConfigMap mais con√ßu pour stocker des donn√©es sensibles (mots de passe, tokens, cl√©s SSH).

**Caract√©ristiques des Secrets :**
- Donn√©es encod√©es en base64 (pas chiffr√©es !)
- Peuvent √™tre mont√©s en m√©moire (tmpfs) dans les Pods
- Acc√®s contr√¥l√© par RBAC
- Ne sont pas logg√©s par d√©faut

**Cr√©ation d'un Secret :**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
data:
  # Les valeurs doivent √™tre en base64
  username: YWRtaW4=  # "admin" en base64
  password: cEBzc3cwcmQ=  # "p@ssw0rd" en base64
```

**Ou plus simplement avec stringData :**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
stringData:
  username: admin
  password: p@ssw0rd
```

### 7. PersistentVolume et PersistentVolumeClaim

Pour les applications qui ont besoin de stocker des donn√©es de mani√®re permanente, Kubernetes offre les volumes persistants.

**PersistentVolume (PV)** : Repr√©sente un espace de stockage physique
**PersistentVolumeClaim (PVC)** : Une demande de stockage par un Pod

**Workflow du stockage persistant :**
1. L'admin cr√©e un PV (ou il est cr√©√© dynamiquement)
2. L'utilisateur cr√©e un PVC pour demander du stockage
3. Kubernetes lie automatiquement le PVC √† un PV compatible
4. Le Pod monte le PVC comme un volume

**Exemple de PVC :**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: webapp-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: microk8s-hostpath  # Classe de stockage MicroK8s
```

**Utilisation dans un Pod :**
```yaml
spec:
  containers:
  - name: webapp
    image: myapp:v1.0
    volumeMounts:
    - name: data-volume
      mountPath: /var/data
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: webapp-storage
```

## Cycle de vie d'un d√©ploiement complet

Voyons maintenant comment tous ces composants s'assemblent dans un d√©ploiement r√©el :

### √âtape 1 : Cr√©ation du Deployment
```bash
kubectl apply -f deployment.yaml
```
- Kubernetes cr√©e le Deployment
- Le Deployment cr√©e un ReplicaSet
- Le ReplicaSet cr√©e les Pods selon le nombre de replicas

### √âtape 2 : Les Pods d√©marrent
- Kubernetes trouve des n≈ìuds avec suffisamment de ressources
- Les images Docker sont t√©l√©charg√©es (pull)
- Les conteneurs d√©marrent
- Les health checks (probes) v√©rifient que l'application est pr√™te

### √âtape 3 : Exposition via Service
```bash
kubectl apply -f service.yaml
```
- Le Service trouve les Pods via les labels
- Une IP stable (ClusterIP) est assign√©e
- Le kube-proxy configure les r√®gles de routage

### √âtape 4 : Acc√®s externe via Ingress
```bash
kubectl apply -f ingress.yaml
```
- L'Ingress Controller d√©tecte la nouvelle r√®gle
- Les certificats SSL sont configur√©s (si sp√©cifi√©s)
- Le routage externe est √©tabli

### √âtape 5 : Configuration et secrets
```bash
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
```
- Les ConfigMaps et Secrets sont cr√©√©s
- Les Pods sont red√©marr√©s si n√©cessaire pour prendre en compte les changements

## Les labels et selectors : le syst√®me nerveux

Les **labels** sont des paires cl√©-valeur attach√©es aux objets Kubernetes. Ils sont cruciaux car ils permettent aux diff√©rents composants de se trouver.

**Exemple de flux avec labels :**
```yaml
# Deployment
metadata:
  labels:
    app: webapp
    version: v1
    environment: lab

# Service (trouve les Pods via selector)
spec:
  selector:
    app: webapp

# Ingress (trouve le Service par son nom)
backend:
  service:
    name: webapp-service
```

**Bonnes pratiques pour les labels :**
- `app` : nom de l'application
- `version` : version de l'application
- `environment` : dev, test, staging, prod
- `component` : frontend, backend, database
- `managed-by` : outil de d√©ploiement (kubectl, helm, etc.)

## Health checks : la surveillance de sant√©

Kubernetes surveille constamment la sant√© de vos Pods via des **probes** :

### Liveness Probe
V√©rifie si le conteneur est vivant. S'il √©choue, Kubernetes red√©marre le conteneur.

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
```

### Readiness Probe
V√©rifie si le conteneur est pr√™t √† recevoir du trafic. S'il √©choue, le Pod est retir√© du Service.

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
```

### Startup Probe
Pour les applications avec un d√©marrage lent, √©vite que les autres probes √©chouent pendant le d√©marrage.

```yaml
startupProbe:
  httpGet:
    path: /started
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```

## Strat√©gies de d√©ploiement

Le Deployment offre plusieurs strat√©gies pour mettre √† jour vos applications :

### RollingUpdate (par d√©faut)
Remplace progressivement les anciens Pods par les nouveaux.

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Nombre max de Pods en plus
      maxUnavailable: 1  # Nombre max de Pods indisponibles
```

**Processus :**
1. Cr√©e un nouveau Pod avec la nouvelle version
2. Attend qu'il soit Ready
3. Supprime un ancien Pod
4. R√©p√®te jusqu'√† ce que tous soient mis √† jour

### Recreate
Supprime tous les anciens Pods avant de cr√©er les nouveaux (downtime).

```yaml
spec:
  strategy:
    type: Recreate
```

**Utilisation :** Pour les applications qui ne peuvent pas avoir plusieurs versions en m√™me temps.

## Gestion des ressources

D√©finir les ressources est crucial pour la stabilit√© du cluster :

### Requests
Les ressources minimum garanties pour le Pod.

### Limits
Les ressources maximum que le Pod peut utiliser.

```yaml
resources:
  requests:
    memory: "128Mi"  # 128 m√©gaoctets
    cpu: "250m"      # 0.25 CPU
  limits:
    memory: "512Mi"
    cpu: "1000m"     # 1 CPU
```

**Unit√©s de mesure :**
- CPU : en millicores (1000m = 1 CPU)
- M√©moire : en bytes (Ki, Mi, Gi)

## Namespaces : l'organisation logique

Les **Namespaces** permettent d'organiser et d'isoler les ressources :

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: mon-projet
```

**Namespaces par d√©faut :**
- `default` : namespace par d√©faut
- `kube-system` : composants syst√®me de Kubernetes
- `kube-public` : ressources publiques
- `kube-node-lease` : infos de sant√© des n≈ìuds

**Utilisation :**
```bash
# Cr√©er des ressources dans un namespace
kubectl apply -f deployment.yaml -n mon-projet

# Changer le namespace par d√©faut
kubectl config set-context --current --namespace=mon-projet
```

## Architecture compl√®te d'un d√©ploiement

Voici comment tous ces √©l√©ments s'assemblent dans une application r√©elle :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Cluster Kubernetes              ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                Namespace: mon-app              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ConfigMap   ‚îÇ  ‚îÇ   Secret    ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                 ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ         Deployment             ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     replicas: 3                ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ   ReplicaSet   ‚îÇ                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚ñº           ‚ñº           ‚ñº                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Pod1 ‚îÇ   ‚îÇ Pod2 ‚îÇ   ‚îÇ Pod3 ‚îÇ                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îÇ          ‚îÇ          ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                ‚ñº                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ   Service   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ ClusterIP   ‚îÇ         ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                 ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                          ‚îÇ   Ingress   ‚îÇ       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                                 Internet
```

## Points cl√©s √† retenir

1. **Hi√©rarchie des objets** : Deployment ‚Üí ReplicaSet ‚Üí Pod ‚Üí Container

2. **S√©paration des responsabilit√©s** :
   - Deployment : gestion du cycle de vie
   - Service : r√©seau stable
   - Ingress : acc√®s externe
   - ConfigMap/Secret : configuration
   - PV/PVC : stockage

3. **Les labels sont essentiels** : Ils permettent aux composants de se trouver

4. **Tout est d√©claratif** : Vous d√©crivez l'√©tat d√©sir√©, Kubernetes s'occupe du reste

5. **La r√©silience est int√©gr√©e** : Auto-healing, rolling updates, load balancing

6. **L'isolation par namespace** : Organisez vos applications logiquement

## Conclusion

Comprendre l'anatomie d'un d√©ploiement Kubernetes est fondamental pour utiliser efficacement votre cluster MicroK8s. Chaque composant a un r√¥le sp√©cifique, et ensemble ils cr√©ent un syst√®me robuste et scalable pour vos applications.

Dans la prochaine section (7.2), nous verrons comment √©crire des manifestes YAML efficaces pour d√©finir tous ces composants. Puis nous mettrons en pratique ces concepts en d√©ployant une vraie application (7.3).

N'oubliez pas : dans votre lab personnel, vous avez la libert√© d'exp√©rimenter. Cr√©ez, cassez, recr√©ez. C'est en manipulant ces objets que vous d√©velopperez une compr√©hension intuitive de Kubernetes.

‚è≠Ô∏è
