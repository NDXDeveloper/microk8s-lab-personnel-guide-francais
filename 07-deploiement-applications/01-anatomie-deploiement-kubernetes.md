ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 7.1 - Anatomie d'un dÃ©ploiement Kubernetes

## Introduction

Un dÃ©ploiement Kubernetes est comme un orchestre symphonique : plusieurs composants travaillent ensemble harmonieusement pour faire fonctionner votre application. Comprendre comment ces piÃ¨ces s'assemblent est essentiel pour maÃ®triser Kubernetes. Dans cette section, nous allons dissÃ©quer un dÃ©ploiement typique pour comprendre chaque composant et son rÃ´le.

## Vue d'ensemble de l'architecture

Quand vous dÃ©ployez une application sur Kubernetes, vous ne crÃ©ez pas simplement un conteneur qui tourne. Vous mettez en place tout un Ã©cosystÃ¨me de ressources qui collaborent :

```
Internet â†’ Ingress â†’ Service â†’ Deployment â†’ ReplicaSet â†’ Pod(s) â†’ Container(s)
                        â†“
                  ConfigMap/Secret
                        â†“
                 PersistentVolume
```

Chaque flÃ¨che reprÃ©sente une relation ou un flux de communication. Voyons maintenant chaque composant en dÃ©tail.

## Les composants fondamentaux

### 1. Le Pod : l'unitÃ© de base

Le **Pod** est l'unitÃ© atomique de Kubernetes, la plus petite chose que vous pouvez dÃ©ployer. Pensez Ã  un Pod comme une "machine virtuelle logique" qui contient :

- **Un ou plusieurs conteneurs** (gÃ©nÃ©ralement un seul)
- **Un rÃ©seau partagÃ©** (mÃªme adresse IP pour tous les conteneurs du Pod)
- **Un stockage partagÃ©** (volumes accessibles par tous les conteneurs)
- **Une identitÃ© unique** dans le cluster

**CaractÃ©ristiques importantes d'un Pod :**
- Il est Ã©phÃ©mÃ¨re par nature (peut disparaÃ®tre Ã  tout moment)
- Il a une seule adresse IP
- Les conteneurs Ã  l'intÃ©rieur peuvent communiquer via `localhost`
- Il vit sur un seul nÅ“ud (ne peut pas Ãªtre rÃ©parti sur plusieurs machines)

**Exemple de dÃ©finition d'un Pod simple :**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mon-application
  labels:
    app: webapp
spec:
  containers:
  - name: app-container
    image: nginx:latest
    ports:
    - containerPort: 80
```

**Pourquoi on ne dÃ©ploie jamais directement des Pods :**
En pratique, vous ne crÃ©ez presque jamais de Pods directement. Pourquoi ? Parce qu'ils sont mortels. Si un Pod crash ou si le nÅ“ud sur lequel il tourne a un problÃ¨me, le Pod disparaÃ®t et ne revient pas. C'est lÃ  qu'intervient le Deployment.

### 2. Le Deployment : le gestionnaire intelligent

Le **Deployment** est votre chef d'orchestre. Il s'assure que votre application tourne toujours avec le bon nombre d'instances (replicas) et gÃ¨re les mises Ã  jour.

**ResponsabilitÃ©s du Deployment :**
- Maintenir le nombre dÃ©sirÃ© de Pods en fonctionnement
- Remplacer les Pods dÃ©faillants automatiquement
- GÃ©rer les montÃ©es de version (rolling updates)
- Permettre les rollbacks en cas de problÃ¨me
- Distribuer les Pods sur diffÃ©rents nÅ“uds pour la rÃ©silience

**Structure d'un Deployment :**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
  labels:
    app: webapp
spec:
  replicas: 3  # Nombre de Pods dÃ©sirÃ©s
  selector:
    matchLabels:
      app: webapp  # SÃ©lectionne les Pods avec ce label
  template:
    metadata:
      labels:
        app: webapp  # Labels appliquÃ©s aux Pods crÃ©Ã©s
    spec:
      containers:
      - name: webapp
        image: myapp:v1.0
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
```

**Le ReplicaSet : le bras droit du Deployment**

Quand vous crÃ©ez un Deployment, il crÃ©e automatiquement un **ReplicaSet**. Le ReplicaSet est responsable de maintenir le nombre exact de Pods spÃ©cifiÃ©. Si vous demandez 3 replicas et qu'un Pod meurt, le ReplicaSet en crÃ©e immÃ©diatement un nouveau.

Vous n'interagissez gÃ©nÃ©ralement pas directement avec les ReplicaSets, mais il est important de savoir qu'ils existent pour comprendre la hiÃ©rarchie :
- Deployment â†’ gÃ¨re â†’ ReplicaSet â†’ gÃ¨re â†’ Pods

### 3. Le Service : l'annuaire tÃ©lÃ©phonique

Les Pods sont Ã©phÃ©mÃ¨res et leurs IPs changent constamment. Comment faire pour accÃ©der Ã  votre application de maniÃ¨re stable ? C'est le rÃ´le du **Service**.

**Le Service offre :**
- Une adresse IP stable (ClusterIP) qui ne change jamais
- Un nom DNS interne (ex: `webapp-service.default.svc.cluster.local`)
- Du load balancing automatique entre les Pods
- La dÃ©couverte de service (service discovery)

**Types de Services :**

**ClusterIP (par dÃ©faut)** : Accessible uniquement depuis l'intÃ©rieur du cluster
```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: ClusterIP
  selector:
    app: webapp  # SÃ©lectionne les Pods avec ce label
  ports:
  - port: 80        # Port du Service
    targetPort: 8080 # Port du conteneur
    protocol: TCP
```

**NodePort** : Expose le service sur un port de chaque nÅ“ud (30000-32767)
```yaml
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080  # Accessible via <IP-du-nÅ“ud>:30080
```

**LoadBalancer** : Demande un load balancer externe (cloud provider ou MetalLB)
```yaml
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
```

### 4. L'Ingress : la porte d'entrÃ©e

L'**Ingress** est votre rÃ©ceptionniste. Il reÃ§oit le trafic HTTP/HTTPS externe et le dirige vers les bons Services basÃ© sur des rÃ¨gles (nom de domaine, chemin URL).

**FonctionnalitÃ©s de l'Ingress :**
- Routage basÃ© sur le nom d'hÃ´te (`app1.example.com` â†’ Service A)
- Routage basÃ© sur le chemin (`/api` â†’ Service API, `/web` â†’ Service Web)
- Terminaison SSL/TLS (HTTPS)
- Load balancing
- HÃ´tes virtuels

**Exemple d'Ingress :**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: webapp-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt"
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - webapp.monlab.local
    secretName: webapp-tls
  rules:
  - host: webapp.monlab.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: webapp-service
            port:
              number: 80
```

### 5. ConfigMap : la configuration externalisÃ©e

Un **ConfigMap** stocke des donnÃ©es de configuration non sensibles que vos applications peuvent utiliser.

**UtilitÃ©s du ConfigMap :**
- Fichiers de configuration
- Variables d'environnement
- Arguments de ligne de commande
- URLs d'API, paramÃ¨tres d'application

**CrÃ©ation d'un ConfigMap :**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
data:
  # PropriÃ©tÃ©s simples
  database_url: "postgres://db.monlab.local:5432/myapp"
  log_level: "info"

  # Fichier de configuration complet
  app.properties: |
    server.port=8080
    app.name=MonApplication
    cache.enabled=true
```

**Utilisation dans un Pod :**
```yaml
spec:
  containers:
  - name: webapp
    image: myapp:v1.0
    env:
    - name: DATABASE_URL
      valueFrom:
        configMapKeyRef:
          name: webapp-config
          key: database_url
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    configMap:
      name: webapp-config
```

### 6. Secret : les donnÃ©es sensibles

Un **Secret** est similaire Ã  un ConfigMap mais conÃ§u pour stocker des donnÃ©es sensibles (mots de passe, tokens, clÃ©s SSH).

**CaractÃ©ristiques des Secrets :**
- DonnÃ©es encodÃ©es en base64 (pas chiffrÃ©es !)
- Peuvent Ãªtre montÃ©s en mÃ©moire (tmpfs) dans les Pods
- AccÃ¨s contrÃ´lÃ© par RBAC
- Ne sont pas loggÃ©s par dÃ©faut

**CrÃ©ation d'un Secret :**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
data:
  # Les valeurs doivent Ãªtre en base64
  username: YWRtaW4=  # "admin" en base64
  password: cEBzc3cwcmQ=  # "p@ssw0rd" en base64
```

**Ou plus simplement avec stringData :**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: webapp-secret
type: Opaque
stringData:
  username: admin
  password: p@ssw0rd
```

### 7. PersistentVolume et PersistentVolumeClaim

Pour les applications qui ont besoin de stocker des donnÃ©es de maniÃ¨re permanente, Kubernetes offre les volumes persistants.

**PersistentVolume (PV)** : ReprÃ©sente un espace de stockage physique
**PersistentVolumeClaim (PVC)** : Une demande de stockage par un Pod

**Workflow du stockage persistant :**
1. L'admin crÃ©e un PV (ou il est crÃ©Ã© dynamiquement)
2. L'utilisateur crÃ©e un PVC pour demander du stockage
3. Kubernetes lie automatiquement le PVC Ã  un PV compatible
4. Le Pod monte le PVC comme un volume

**Exemple de PVC :**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: webapp-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: microk8s-hostpath  # Classe de stockage MicroK8s
```

**Utilisation dans un Pod :**
```yaml
spec:
  containers:
  - name: webapp
    image: myapp:v1.0
    volumeMounts:
    - name: data-volume
      mountPath: /var/data
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: webapp-storage
```

## Cycle de vie d'un dÃ©ploiement complet

Voyons maintenant comment tous ces composants s'assemblent dans un dÃ©ploiement rÃ©el :

### Ã‰tape 1 : CrÃ©ation du Deployment
```bash
kubectl apply -f deployment.yaml
```
- Kubernetes crÃ©e le Deployment
- Le Deployment crÃ©e un ReplicaSet
- Le ReplicaSet crÃ©e les Pods selon le nombre de replicas

### Ã‰tape 2 : Les Pods dÃ©marrent
- Kubernetes trouve des nÅ“uds avec suffisamment de ressources
- Les images Docker sont tÃ©lÃ©chargÃ©es (pull)
- Les conteneurs dÃ©marrent
- Les health checks (probes) vÃ©rifient que l'application est prÃªte

### Ã‰tape 3 : Exposition via Service
```bash
kubectl apply -f service.yaml
```
- Le Service trouve les Pods via les labels
- Une IP stable (ClusterIP) est assignÃ©e
- Le kube-proxy configure les rÃ¨gles de routage

### Ã‰tape 4 : AccÃ¨s externe via Ingress
```bash
kubectl apply -f ingress.yaml
```
- L'Ingress Controller dÃ©tecte la nouvelle rÃ¨gle
- Les certificats SSL sont configurÃ©s (si spÃ©cifiÃ©s)
- Le routage externe est Ã©tabli

### Ã‰tape 5 : Configuration et secrets
```bash
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
```
- Les ConfigMaps et Secrets sont crÃ©Ã©s
- Les Pods sont redÃ©marrÃ©s si nÃ©cessaire pour prendre en compte les changements

## Les labels et selectors : le systÃ¨me nerveux

Les **labels** sont des paires clÃ©-valeur attachÃ©es aux objets Kubernetes. Ils sont cruciaux car ils permettent aux diffÃ©rents composants de se trouver.

**Exemple de flux avec labels :**
```yaml
# Deployment
metadata:
  labels:
    app: webapp
    version: v1
    environment: lab

# Service (trouve les Pods via selector)
spec:
  selector:
    app: webapp

# Ingress (trouve le Service par son nom)
backend:
  service:
    name: webapp-service
```

**Bonnes pratiques pour les labels :**
- `app` : nom de l'application
- `version` : version de l'application
- `environment` : dev, test, staging, prod
- `component` : frontend, backend, database
- `managed-by` : outil de dÃ©ploiement (kubectl, helm, etc.)

## Health checks : la surveillance de santÃ©

Kubernetes surveille constamment la santÃ© de vos Pods via des **probes** :

### Liveness Probe
VÃ©rifie si le conteneur est vivant. S'il Ã©choue, Kubernetes redÃ©marre le conteneur.

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
```

### Readiness Probe
VÃ©rifie si le conteneur est prÃªt Ã  recevoir du trafic. S'il Ã©choue, le Pod est retirÃ© du Service.

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
```

### Startup Probe
Pour les applications avec un dÃ©marrage lent, Ã©vite que les autres probes Ã©chouent pendant le dÃ©marrage.

```yaml
startupProbe:
  httpGet:
    path: /started
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```

## StratÃ©gies de dÃ©ploiement

Le Deployment offre plusieurs stratÃ©gies pour mettre Ã  jour vos applications :

### RollingUpdate (par dÃ©faut)
Remplace progressivement les anciens Pods par les nouveaux.

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Nombre max de Pods en plus
      maxUnavailable: 1  # Nombre max de Pods indisponibles
```

**Processus :**
1. CrÃ©e un nouveau Pod avec la nouvelle version
2. Attend qu'il soit Ready
3. Supprime un ancien Pod
4. RÃ©pÃ¨te jusqu'Ã  ce que tous soient mis Ã  jour

### Recreate
Supprime tous les anciens Pods avant de crÃ©er les nouveaux (downtime).

```yaml
spec:
  strategy:
    type: Recreate
```

**Utilisation :** Pour les applications qui ne peuvent pas avoir plusieurs versions en mÃªme temps.

## Gestion des ressources

DÃ©finir les ressources est crucial pour la stabilitÃ© du cluster :

### Requests
Les ressources minimum garanties pour le Pod.

### Limits
Les ressources maximum que le Pod peut utiliser.

```yaml
resources:
  requests:
    memory: "128Mi"  # 128 mÃ©gaoctets
    cpu: "250m"      # 0.25 CPU
  limits:
    memory: "512Mi"
    cpu: "1000m"     # 1 CPU
```

**UnitÃ©s de mesure :**
- CPU : en millicores (1000m = 1 CPU)
- MÃ©moire : en bytes (Ki, Mi, Gi)

## Namespaces : l'organisation logique

Les **Namespaces** permettent d'organiser et d'isoler les ressources :

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: mon-projet
```

**Namespaces par dÃ©faut :**
- `default` : namespace par dÃ©faut
- `kube-system` : composants systÃ¨me de Kubernetes
- `kube-public` : ressources publiques
- `kube-node-lease` : infos de santÃ© des nÅ“uds

**Utilisation :**
```bash
# CrÃ©er des ressources dans un namespace
kubectl apply -f deployment.yaml -n mon-projet

# Changer le namespace par dÃ©faut
kubectl config set-context --current --namespace=mon-projet
```

## Architecture complÃ¨te d'un dÃ©ploiement

Voici comment tous ces Ã©lÃ©ments s'assemblent dans une application rÃ©elle :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Cluster Kubernetes              â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                Namespace: mon-app              â”‚  â”‚
â”‚  â”‚                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚  â”‚ ConfigMap   â”‚  â”‚   Secret    â”‚              â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚         â”‚                 â”‚                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚         Deployment             â”‚            â”‚  â”‚
â”‚  â”‚  â”‚     replicas: 3                â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â”‚                 â”‚                              â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚  â”‚
â”‚  â”‚         â”‚   ReplicaSet   â”‚                     â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚  â”‚
â”‚  â”‚                 â”‚                              â”‚  â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚
â”‚  â”‚     â–¼           â–¼           â–¼                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”                â”‚  â”‚
â”‚  â”‚  â”‚ Pod1 â”‚   â”‚ Pod2 â”‚   â”‚ Pod3 â”‚                â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜                â”‚  â”‚
â”‚  â”‚     â”‚          â”‚          â”‚                    â”‚  â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â”‚                â–¼                               â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚  â”‚
â”‚  â”‚         â”‚   Service   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚         â”‚ ClusterIP   â”‚         â”‚              â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚              â”‚  â”‚
â”‚  â”‚                                 â”‚              â”‚  â”‚
â”‚  â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”‚  â”‚
â”‚  â”‚                          â”‚   Ingress   â”‚       â”‚  â”‚
â”‚  â”‚                          â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                 Internet
```

## Points clÃ©s Ã  retenir

1. **HiÃ©rarchie des objets** : Deployment â†’ ReplicaSet â†’ Pod â†’ Container

2. **SÃ©paration des responsabilitÃ©s** :
   - Deployment : gestion du cycle de vie
   - Service : rÃ©seau stable
   - Ingress : accÃ¨s externe
   - ConfigMap/Secret : configuration
   - PV/PVC : stockage

3. **Les labels sont essentiels** : Ils permettent aux composants de se trouver

4. **Tout est dÃ©claratif** : Vous dÃ©crivez l'Ã©tat dÃ©sirÃ©, Kubernetes s'occupe du reste

5. **La rÃ©silience est intÃ©grÃ©e** : Auto-healing, rolling updates, load balancing

6. **L'isolation par namespace** : Organisez vos applications logiquement

## Conclusion

Comprendre l'anatomie d'un dÃ©ploiement Kubernetes est fondamental pour utiliser efficacement votre cluster MicroK8s. Chaque composant a un rÃ´le spÃ©cifique, et ensemble ils crÃ©ent un systÃ¨me robuste et scalable pour vos applications.

Dans la prochaine section (7.2), nous verrons comment Ã©crire des manifestes YAML efficaces pour dÃ©finir tous ces composants. Puis nous mettrons en pratique ces concepts en dÃ©ployant une vraie application (7.3).

N'oubliez pas : dans votre lab personnel, vous avez la libertÃ© d'expÃ©rimenter. CrÃ©ez, cassez, recrÃ©ez. C'est en manipulant ces objets que vous dÃ©velopperez une comprÃ©hension intuitive de Kubernetes.

â­ï¸
